Download Link: https://assignmentchef.com/product/solved-comp135-assignment-5
<br>
For this assignment, you will make modifications to a Python notebook (hw05.ipynb) that has been supplied. You will complete the various required sections outlined below in that notebook. When you are done, generate a PDF version of that notebook, with all results (figures, printed results, etc.) included, for submission to Gradescope. You will also submit the raw notebook source, two PDF images generated by your program (but not embedded in the worksheet itself), and a COLLABORATORS.txt file, via a separate link, as described below.

<h1>Adding the graphviz library</h1>

Before you start the assignment, you should add one additional library to your Python install; this will allow you to visualize decision trees in graphical format. You will only need to do the following once; after these steps are performed, these tools will be available, along with all the other ones we have been using, simply by activating the environment as usual.

<ol>

 <li>Activate your environment as you normally would:</li>

</ol>

conda activate ml135_env_sp21

<ol start="2">

 <li>From within the environment, install graphviz:</li>

</ol>

<h2>conda install python-graphviz</h2>

Once these steps are completed, you will be able to use the new tools from inside the COMP 135 environment, in notebooks and other Python code.

<h1>Decision Trees</h1>

You will examine the use of decision trees for classification, along with different heuristics for evaluating which features to choose when building such trees.

<table width="377">

 <tbody>

  <tr>

   <td width="238">o o                        o o x x x x</td>

   <td width="139">o o o                          o x                           x x x</td>

  </tr>

 </tbody>

</table>

<ol>

 <li>The diagram above shows the results of splitting a simple data-set according to two binary features (<em>A </em>and <em>B</em>). The data-set consists of eight entries, of two different types (o and x). You will compute and display the results of computing the two feature-heuristics seen in the readings and in class lecture notes.</li>

</ol>

<ul>

 <li>Compute the values for each feature, based upon the counting heuristic discussed in the reading (Daum´e). Print out the features in order from best to worst, along with the heuristic (correctness) value for that feature, using the format:</li>

</ul>

feature_name: num_correct/total_data

<ul>

 <li>Compute the values for each feature, based upon the information-theoretic heuristic discussed in lecture. Print out the features in order from best to worst, along with the heuristic (gain) value for that feature, to 3 decimal places of precision, using the format:</li>

</ul>

feature_name: information_gain

<ul>

 <li>Discuss the results: if we built a tree using each of these heuristics, what would happen? What does this mean?</li>

</ul>

<ol start="2">

 <li>We have provided some data on abalone, a widespread shellfish.<sup>∗ </sup>The input data consists of a number of features of abalone, as shown in the following table, while the output is the number of rings found in the abalone shell:</li>

</ol>

<table width="454">

 <tbody>

  <tr>

   <td width="138"><strong>column</strong></td>

   <td width="66"><strong>type</strong></td>

   <td width="48"><strong>unit</strong></td>

   <td width="201"><strong>description</strong></td>

  </tr>

  <tr>

   <td width="138">is_male</td>

   <td width="66">binary</td>

   <td width="48"> </td>

   <td width="201">1 == male; 0 == female</td>

  </tr>

  <tr>

   <td width="138">length_mm</td>

   <td width="66">numeric</td>

   <td width="48">mm</td>

   <td width="201">longest shell measurement</td>

  </tr>

  <tr>

   <td width="138">diam_mm</td>

   <td width="66">numeric</td>

   <td width="48">mm</td>

   <td width="201">shell diameter, perpendicular</td>

  </tr>

  <tr>

   <td width="138">height_mm</td>

   <td width="66">numeric</td>

   <td width="48">mm</td>

   <td width="201">height of shell</td>

  </tr>

  <tr>

   <td width="138">whole_weight_g</td>

   <td width="66">numeric</td>

   <td width="48">gram</td>

   <td width="201">weight (entire)</td>

  </tr>

  <tr>

   <td width="138">shucked_weight_g</td>

   <td width="66">numeric</td>

   <td width="48">gram</td>

   <td width="201">weight (meat)</td>

  </tr>

  <tr>

   <td width="138">viscera_weight_g</td>

   <td width="66">numeric</td>

   <td width="48">gram</td>

   <td width="201">weight (guts)</td>

  </tr>

  <tr>

   <td width="138">shell_weight_g</td>

   <td width="66">numeric</td>

   <td width="48">gram</td>

   <td width="201">weight (dried shell)</td>

  </tr>

 </tbody>

</table>

In addition, we have supplied a simplified version of the data, where each input-feature has been converted to a binary value (either above average value for that feature (1), or not 0), and the output value <em>y </em>∈{0<em>,</em>1<em>,</em>2} no signifies a <em>Small</em>, <em>Medium</em>, or <em>Large </em>number of rings; the data has also been simplified down to only four features of the original eight. Each data-set is broken up into x and y sets already, for both training and testing.

Your code will explore these data sets, computing the two heuristics for the simplified data, and classifying both sets using decision trees.

<ul>

 <li>Compute the counting-based heuristic for the features of the <em>simplified </em>abalone data. Print out the features in order, using the same format as before.</li>

 <li>Compute the information-theoretic heuristic for the features of the <em>simplified </em>abalone data. Print out the features in order, using the same format as before.</li>

</ul>

∗

Original data: Warwick J. Nash, et al. (1994) <a href="https://archive.ics.uci.edu/ml/datasets/Abalone">https://archive.ics.uci.edu/ml/datasets/Abalone</a>

<ol start="3">

 <li>You will use sklearn decision trees on both versions of the abalone data-set:</li>

</ol>

<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.tree. </a><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">DecisionTreeClassifier.html</a>

<ul>

 <li>For each data-set, create the classifier using the criterion=’entropy’ option, which uses the same information-theoretic heuristic discussed in lecture. After building the model, you can use its score() function to get its accuracy on each of the testing and training portions of the data. Print out these values, being clear which value is which.</li>

</ul>

In addition, export the two trees as PDF images, using the export_graphviz() and render() functions.<sup>† </sup>When done, you should be able to open those images (they will be in the directory with your active notebook file) to examine them.

<ul>

 <li>Discuss the results you have just seen. What do the various accuracy-score values tell you? How do the two trees that are produced differ? Looking at the outputs (leaves) of the simplified-data tree, what sorts of errors does that tree make?</li>

</ul>